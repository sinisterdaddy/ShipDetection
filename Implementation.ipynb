{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing of the Dataset\n",
    "\n",
    "```\n",
    "!rm -r ~/.kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!mv ./kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets list\n",
    "!kaggle competitions download -c airbus-ship-detection -p /content/MyDrive/MyDrive/S22Datasets/ --force\n",
    "!unzip /content/MyDrive/MyDrive/S22Datasets/airbus-ship-detection.zip -d /content/MyDrive/MyDrive/S22Datasets/\n",
    "```\n",
    "\n",
    "Suggestion:\n",
    "Click on the shared folder ```S22Datasets``` and select ```add shortcut to drive``` to make all paths consistent to everyone using this notebook (Applicable to Jay and Ignat)\n",
    "\n",
    "#### Python Configuration:\n",
    "```{python}\n",
    "#Configuration environment\n",
    "import os\n",
    "data = {\"username\": <user_name>,\"key\": <api_key>}\n",
    "os.environ['KAGGLE_USERNAME'] = data[\"username\"] # username from the json file\n",
    "os.environ['KAGGLE_KEY'] = data[\"key\"] # key from the json file\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "import os \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\n",
    "import skimage\n",
    "from skimage.io import imread\n",
    "from skimage.util import montage as montage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Since all the coding was done on a colab notebook, we need to mount the google drive to access the data\n",
    "drive.mount(\"/content/drive\", force_remount= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data directory\n",
    "base_path = \"/content/drive/MyDrive/S22Datasets/\"\n",
    "train_path = \"/content/drive/MyDrive/S22Datasets/train_v2\"\n",
    "test_path = \"/content/drive/MyDrive/S22Datasets/test_v2\"\n",
    "\n",
    "# Getting a count of number of trainign and testing images from the airbus ship detection dataset\n",
    "train_data = os.listdir(train_path)\n",
    "test_data = os.listdir(test_path)\n",
    "\n",
    "print(\"Train Images : {} \\nTest Images : {}\".format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the map in for the labels\n",
    "segmentation_data_df = pd.read_csv(base_path + \"train_ship_segmentations_v2.csv\")\n",
    "\n",
    "# Filtering out data based on our available sample \n",
    "true_segment_df = segmentation_data_df[segmentation_data_df[\"ImageId\"].isin(train_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting a train and test split \n",
    "df = true_segment_df\n",
    "total = len(df)\n",
    "\n",
    "# splitting in a 70/30 ratio\n",
    "train_ids, valid_ids = train_test_split(df.index, \n",
    "                 test_size = 0.3, \n",
    "                 stratify = df['HasShip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the distribution of the labels\n",
    "total = len(df)\n",
    "ship = df['HasShip'].sum()\n",
    "no_ship = total - ship\n",
    "total_ships = int(df['TotalShips'].sum())\n",
    "    \n",
    "print(f\"Images: {total} \\nShips:  {total_ships}\")\n",
    "print(f\"Images with ships:    {round(ship/total,2)} ({ship})\")\n",
    "print(f\"Images with no ships: {round(no_ship/total,2)} ({no_ship})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using keras ImageDataGenerator to preprocess the images\n",
    "training_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path, \n",
    "    labels = None, \n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 30,\n",
    "    image_size = (256,256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate maps for the training and validation data\n",
    "train_df = df[df.index.isin(train_ids)]\n",
    "valid_df = df[df.index.isin(valid_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininng required and optional parameters for the ImageDataGenerator for the training and testing \n",
    "# dataset\n",
    "dg_args = dict(featurewise_center = False, \n",
    "                  samplewise_center = False,\n",
    "                  rotation_range = 45, \n",
    "                  width_shift_range = 0.1, \n",
    "                  height_shift_range = 0.1, \n",
    "                  shear_range = 0.01,\n",
    "                  zoom_range = [0.9, 1.25],  \n",
    "                  brightness_range = [0.5, 1.5],\n",
    "                  horizontal_flip = True, \n",
    "                  vertical_flip = True,\n",
    "                  fill_mode = 'reflect',\n",
    "                   data_format = 'channels_last',\n",
    "              preprocessing_function = preprocess_input)\n",
    "\n",
    "valid_args = dict(fill_mode = 'reflect',\n",
    "                   data_format = 'channels_last',\n",
    "                  preprocessing_function = preprocess_input)\n",
    "\n",
    "core_idg = ImageDataGenerator(**dg_args)\n",
    "valid_idg = ImageDataGenerator(**valid_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n",
    "    \"\"\"\n",
    "    Map the images to their actual labels stored in the segmentation_df\n",
    "\n",
    "    Args:\n",
    "        img_data_gen (iterator): Generator object for the training dataset\n",
    "        in_df (pd.DataFrame): Map of the images to their labels\n",
    "        path_col (pd.Series): Column in the dataframe that contains the path to the image\n",
    "        y_col (pd.Series): Label Column\n",
    "\n",
    "    Returns:\n",
    "        pd.Dataframe : Mapped images to their labels accessed using generators \n",
    "    \"\"\"\n",
    "    base_dir = base_path\n",
    "    print('## Ignore next message from keras, values are replaced anyways')\n",
    "    \n",
    "    # Loading the images from the dataframe\n",
    "    df_gen = img_data_gen.flow_from_directory(base_dir, \n",
    "                                     class_mode = 'sparse',\n",
    "                                    **dflow_args)\n",
    "    # Get file names\n",
    "    df_gen.filenames = in_df.index\n",
    "    # Get labels\n",
    "    df_gen.classes = np.stack(in_df[y_col].values)\n",
    "    # Sample size\n",
    "    df_gen.samples = in_df.shape[0]\n",
    "    df_gen.n = in_df.shape[0]\n",
    "    df_gen._set_index_array()\n",
    "    df_gen.directory = '' # since we have the full path\n",
    "    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n",
    "    return df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the trainign and validation generators\n",
    "train_gen = flow_from_dataframe(core_idg, train_df, \n",
    "                             path_col = 'ImageId',\n",
    "                            y_col = 'HasShip', \n",
    "                            target_size = (128,128),\n",
    "                             color_mode = 'rgb',\n",
    "                            batch_size = 100)\n",
    "\n",
    "# used a fixed dataset for evaluating the algorithm\n",
    "valid_x, valid_y = next(flow_from_dataframe(valid_idg, \n",
    "                               valid_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'HasShip', \n",
    "                            target_size = (128,128),\n",
    "                             color_mode = 'rgb',\n",
    "                            batch_size = 100)) # one big batch\n",
    "print(valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a montage of images\n",
    "montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n",
    "RGB_FLIP = 1\n",
    "\n",
    "# Getting training and testing data\n",
    "t_x, t_y = next(train_gen)\n",
    "print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
    "ax1.imshow(montage_rgb((t_x - t_x.min()) / (t_x.max() - t_x.min()))[:, :, ::RGB_FLIP], cmap='gray')\n",
    "ax1.set_title('images')\n",
    "ax2.plot(t_y)\n",
    "ax2.set_title('ships')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting of the VGG16 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.applications.densenet import DenseNet169, preprocess_input\n",
    "from keras.applications.densenet import DenseNet121, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the required parameters for the model\n",
    "GAUSSIAN_NOISE = 0.1\n",
    "UPSAMPLE_MODE = 'SIMPLE'\n",
    "# number of validation images to use\n",
    "VALID_IMG_COUNT = 1000\n",
    "# maximum number of training images\n",
    "MAX_TRAIN_IMAGES = 8000 \n",
    "IMG_SIZE = (224, 224) # [(224, 224), (384, 384), (512, 512), (640, 640)]\n",
    "BATCH_SIZE = 64 # [1, 8, 16, 24]\n",
    "DROPOUT = 0.5\n",
    "DENSE_COUNT = 128\n",
    "LEARN_RATE = 0.001\n",
    "RGB_FLIP = 1 # should rgb be flipped when rendering images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the model\n",
    "base_pretrained_model = VGG16(input_shape =  t_x.shape[1:], include_top = False, weights = 'imagenet')\n",
    "base_pretrained_model.trainable = False\n",
    "\n",
    "# Adding input and noise layers\n",
    "img_in = keras.layers.Input(t_x.shape[1:], name='Image_RGB_In')\n",
    "img_noise = keras.layers.GaussianNoise(GAUSSIAN_NOISE)(img_in)\n",
    "pt_features = base_pretrained_model(img_noise)\n",
    "pt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\n",
    "bn_features = keras.layers.BatchNormalization()(pt_features)\n",
    "feature_dropout = keras.layers.SpatialDropout2D(DROPOUT)(bn_features)\n",
    "gmp_dr = keras.layers.GlobalMaxPooling2D()(feature_dropout)\n",
    "dr_steps = keras.layers.Dropout(DROPOUT)(keras.layers.Dense(DENSE_COUNT, activation = 'relu')(gmp_dr))\n",
    "out_layer = keras.layers.Dense(1, activation = 'sigmoid')(dr_steps)\n",
    "\n",
    "# Genetating the final model with the modifications \n",
    "ship_model = keras.models.Model(inputs = [img_in], outputs = [out_layer], name = 'full_model')\n",
    "\n",
    "# Compiling the model with Adam optimizer and binary crossentropy loss\n",
    "ship_model.compile(optimizer = keras.optimizers.Adam(learning_rate=LEARN_RATE), loss = 'binary_crossentropy', metrics = ['binary_accuracy'])\n",
    "\n",
    "# Printing out the summary of the model \n",
    "ship_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining various callbacks\n",
    "weight_path=\"{}_weights.best.hdf5\".format('boat_detector')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "train_gen.batch_size = BATCH_SIZE\n",
    "VGG16_history = ship_model.fit_generator(train_gen, \n",
    "                         steps_per_epoch=train_gen.n//BATCH_SIZE,\n",
    "                      validation_data=(valid_x, valid_y), \n",
    "                      epochs=10, \n",
    "                      callbacks=callbacks_list,\n",
    "                      workers=3)\n",
    "\n",
    "# Save the model and its history\n",
    "os.mkdir(base_path + \"VGG16/\")\n",
    "ship_model.save(base_path+ \"VGG16/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the plots \n",
    "plotting_history = VGG16_history.history\n",
    "\n",
    "# Training and validation loss plot \n",
    "ax = plt.subplot(111)\n",
    "ax.plot(plotting_history[\"loss\"], label = \"Training Loss\")\n",
    "ax.plot(plotting_history[\"val_loss\"], label = \"Validation Loss\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(range(0,10))\n",
    "ax.set_xlabel('Epochs', labelpad=15, color='#333333')\n",
    "ax.set_ylabel('Loss', labelpad=15, color='#333333')\n",
    "ax.set_title('Reduction of the Training and Validation Loss', pad=15, color='#333333',\n",
    "             weight='bold')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy plot\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(plotting_history[\"binary_accuracy\"], linestyle = \"--\", marker = 'o', label = \"Training Accuracy\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(range(0,10))\n",
    "ax.set_xlabel('Epochs', labelpad=15, color='#333333')\n",
    "ax.set_ylabel('Accuracy', labelpad=15, color='#333333')\n",
    "ax.set_title('Training and Validation Accuracy trend with increasing epochs', pad=15, color='#333333',\n",
    "             weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting of the VGG19 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the required parameters for the model\n",
    "base_model = VGG19(weights=None, include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.8)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs = base_model.input, outputs = predictions)\n",
    "\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "# Compiling the model with Adam optimizer and binary crossentropy loss\n",
    "model.compile(optimizer=adam, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "vgg19_history = model.fit_generator(train_gen, \n",
    "                         steps_per_epoch=train_gen.n//64,\n",
    "                      validation_data=(valid_x, valid_y), \n",
    "                      epochs=10, \n",
    "                      callbacks=callbacks_list,\n",
    "                      workers=3)\n",
    "\n",
    "# Saving the model and its history\n",
    "plotting_history = pd.DataFrame(vgg19_history.history)\n",
    "plotting_history.to_csv(base_path + \"vgg19.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy plot\n",
    "epochs = np.arange(1, 11)\n",
    "acc = [0.7776, 0.7814, 0.7793, 0.7799, 0.7791, 0.7795, 0.7811, 0.7803, 0.7806, 0.7804]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(epochs, acc, linestyle = \"--\", marker = 'o', label = \"Training Accuracy\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(range(0,10))\n",
    "ax.set_xlabel('Epochs', labelpad=15, color='#333333')\n",
    "ax.set_ylabel('Accuracy', labelpad=15, color='#333333')\n",
    "ax.set_title('Training and Validation Accuracy trend with increasing epochs', pad=15, color='#333333',\n",
    "             weight='bold')\n",
    "plt.savefig('vgg19_acc.png')\n",
    "files.download(\"vgg19_acc.png\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loss plot\n",
    "plt.figure(figsize=(7,5))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(plotting_history[\"loss\"], label = \"Training Loss\")\n",
    "ax.plot(plotting_history[\"val_loss\"], label = \"Validation Loss\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(range(0,10))\n",
    "ax.set_xlabel('Epochs', labelpad=15, color='#333333')\n",
    "ax.set_ylabel('Loss', labelpad=15, color='#333333')\n",
    "ax.set_title('Reduction of the Training and Validation Loss for VGG19', pad=15, color='#333333',\n",
    "             weight='bold')\n",
    "plt.savefig('vgg19_loss.png')\n",
    "files.download(\"vgg19_loss.png\") \n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fititng of the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dropout, Flatten, Dense\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a sequential model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(128, 128, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(128, 128, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(128, 128, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "# Compiling the model with rmsprop optimizer and categorical crossentropy loss\n",
    "model.compile(optimizer='rmsprop', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Printing out the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen.batch_size = 64\n",
    "# Fit the model\n",
    "cnn_history = model.fit_generator(train_gen, \n",
    "                         steps_per_epoch=train_gen.n//64,\n",
    "                      validation_data=(valid_x, valid_y), \n",
    "                      epochs=10, \n",
    "                      callbacks=callbacks_list,\n",
    "                      workers=3)\n",
    "\n",
    "# Save the model and its history\n",
    "model.save(base_path+ \"CNN/\")\n",
    "plotting_history = pd.DataFrame(cnn_history.history)\n",
    "plotting_history.to_csv(base_path + \"cnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loss plot\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(plotting_history[\"loss\"], label = \"Training Loss\")\n",
    "ax.plot(plotting_history[\"val_loss\"], label = \"Validation Loss\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(range(0,10))\n",
    "ax.set_xlabel('Epochs', labelpad=15, color='#333333')\n",
    "ax.set_ylabel('Loss', labelpad=15, color='#333333')\n",
    "ax.set_title('Reduction of the Training and Validation Loss', pad=15, color='#333333',\n",
    "             weight='bold')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy plot\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(plotting_history[\"accuracy\"], linestyle = \"--\", marker = 'o', label = \"Training Accuracy\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(range(0,10))\n",
    "ax.set_xlabel('Epochs', labelpad=15, color='#333333')\n",
    "ax.set_ylabel('Accuracy', labelpad=15, color='#333333')\n",
    "ax.set_title('Training and Validation Accuracy trend with increasing epochs', pad=15, color='#333333',\n",
    "             weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting of the ResNet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the pretrained model\n",
    "base_model =ResNet50(weights= None, include_top=False, input_shape= (128, 128, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.8)(x)\n",
    "predictions = Dense(2, activation= 'softmax')(x)\n",
    "model = Model(inputs = base_model.input, outputs = predictions)\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer= adam, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "resnet_history = model.fit_generator(train_gen, \n",
    "                         steps_per_epoch=train_gen.n//64,\n",
    "                      validation_data=(valid_x, valid_y), \n",
    "                      epochs=10, \n",
    "                      callbacks=callbacks_list,\n",
    "                      workers=3)\n",
    "\n",
    "# Saving the model and its history\n",
    "model.save(base_path + \"ResNet50/\")\n",
    "plotting_history = pd.DataFrame(resnet_history.history)\n",
    "plotting_history.to_csv(base_path + \"resnet50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loss plot\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(plotting_history[\"loss\"], label = \"Training Loss\")\n",
    "ax.plot(plotting_history[\"val_loss\"], label = \"Validation Loss\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(range(0,10))\n",
    "ax.set_xlabel('Epochs', labelpad=15, color='#333333')\n",
    "ax.set_ylabel('Loss', labelpad=15, color='#333333')\n",
    "ax.set_title('Reduction of the Training and Validation Loss', pad=15, color='#333333',\n",
    "             weight='bold')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy plot\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(plotting_history[\"accuracy\"], linestyle = \"--\", marker = 'o', label = \"Training Accuracy\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(range(0,10))\n",
    "ax.set_xlabel('Epochs', labelpad=15, color='#333333')\n",
    "ax.set_ylabel('Accuracy', labelpad=15, color='#333333')\n",
    "ax.set_title('Training and Validation Accuracy trend with increasing epochs', pad=15, color='#333333',\n",
    "             weight='bold')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
